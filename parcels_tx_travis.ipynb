{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597081971890",
   "display_name": "Python 3.7.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Travis Exploration Notebook\n",
    "\n",
    "## Steps:\n",
    "\n",
    "1. Add any required modules to the imports\n",
    "2. Set County specific variables\n",
    "3. Set path to sample dataset(s)\n",
    "4. Import the dataset(s) in the order that they will be merged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import datetime\n",
    "import json\n",
    "import pyspark\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, DoubleType, IntegerType, DateType, TimestampType, BooleanType\n",
    "import quinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set county vars\n",
    "county = \"travis\"\n",
    "state = \"TX\"\n",
    "fips_code = \"48453\"\n",
    "\n",
    "schema_check_db = \"faxdb\"\n",
    "schema_check_coll = \"parcels_validation_test\"\n",
    "\n",
    "county_vars = {\n",
    "    \"county\": county,\n",
    "    \"state\": state,\n",
    "    \"fips_code\": fips_code,\n",
    "    \"pipeline_run_id\": \"987654321\",\n",
    "    \"pipeline_run_time\": datetime.datetime.now(),\n",
    "    \"dataset_date\": datetime.datetime.now(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init spark with delta, mongo support and 16g of memory\n",
    "spark = (pyspark.sql.SparkSession.builder.appName(county)\n",
    "        .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.7.0\")\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\n",
    "            \"spark.sql.catalog.spark_catalog\",\n",
    "            \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "        )\n",
    "        .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.0\")\n",
    "        .config(\"spark.mongodb.input.uri\", f\"mongodb://127.0.0.1/{county}.default\")\n",
    "        .config(\"spark.mongodb.output.uri\", f\"mongodb://127.0.0.1/{county}.default\")\n",
    "        .config(\"spark.driver.memory\", \"16g\")\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample dataset paths (version controlled)\n",
    "sample_ade_info = \"./samples/PROP.TXT\"\n",
    "sample_ade_land = \"./samples/LAND_DET.TXT\"\n",
    "sample_ade_imp_info = \"./samples/IMP_DET.TXT\"\n",
    "# full datasets paths (not version controlled)\n",
    "full_ade_info = \"\"\n",
    "full_ade_land = \"\"\n",
    "full_ade_imp_info = \"\"\n",
    "# set dataset paths\n",
    "path_ade_info = sample_ade_info\n",
    "path_ade_land = sample_ade_land\n",
    "path_ade_imp_info = sample_ade_imp_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load schema path\n",
    "schema_path = f\"./schema_{state}_{county}.json\"\n",
    "with open(schema_path, \"rb\") as s:\n",
    "    schema = StructType.fromJson(json.load(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset APPRAISAL_INFO.TXT\n",
    "def _transform_ade_info():\n",
    "    return (\n",
    "        (spark.read.text(path_ade_info)\n",
    "                .select(\n",
    "                    F.trim(F.col(\"value\").substr(1, 12)).cast(\"string\").alias(\"parcel_id\"),\n",
    "                    F.trim(F.col(\"value\").substr(547,50)).cast(\"string\").alias(\"tax_id\"),\n",
    "                    F.trim(F.col(\"value\").substr(13, 5)).alias(\"type\"),\n",
    "                    F.trim(F.col(\"value\").substr(4460,15)).alias(\"street_number\"),\n",
    "                    F.trim(F.col(\"value\").substr(1040,10)).alias(\"street_pre_direction\"),\n",
    "                    F.trim(F.col(\"value\").substr(1050,50)).alias(\"street_name\"),\n",
    "                    F.trim(F.col(\"value\").substr(1100,10)).alias(\"street_post_direction\"),\n",
    "                    F.trim(F.col(\"value\").substr(4475,5)).alias(\"unit\"),\n",
    "                    F.trim(F.col(\"value\").substr(1110,30)).alias(\"city\"),\n",
    "                    F.trim(F.col(\"value\").substr(1140,10)).alias(\"zip\"),\n",
    "                    F.trim(F.col(\"value\").substr(1150,255)).alias(\"legal_description\"),\n",
    "                    F.trim(F.col(\"value\").substr(1676,10)).alias(\"legal_subdivision\"),\n",
    "                    F.trim(F.col(\"value\").substr(2734,10)).alias(\"improvement_use_code\"),\n",
    "                    (F.trim(F.col(\"value\").substr(2772,20)).cast(\"double\") / 10000).alias(\"lot_size_acres\"),\n",
    "                    F.trim(F.col(\"value\").substr(4214,14)).cast(\"integer\").alias(\"market_value\"),\n",
    "                    F.trim(F.col(\"value\").substr(1946,15)).cast(\"integer\").alias(\"assessed_value\"),\n",
    "                    F.to_timestamp(F.col(\"value\").substr(2034,25), \"MM/dd/yyyy\").alias(\"deed_transfer_date\"),\n",
    "                    F.trim(F.col(\"value\").substr(4492,70)).alias(\"oor_1\"),\n",
    "                    F.trim(F.col(\"value\").substr(4562,60)).alias(\"oor_2\"),\n",
    "                    F.trim(F.col(\"value\").substr(4622,60)).alias(\"oor_3\"),\n",
    "                    F.trim(F.col(\"value\").substr(4682,60)).alias(\"oor_4\"),\n",
    "                    F.trim(F.col(\"value\").substr(4742,50)).alias(\"oor_5\"),\n",
    "                    F.trim(F.col(\"value\").substr(4792,50)).alias(\"oor_6\"),\n",
    "                    F.trim(F.col(\"value\").substr(4847,5)).alias(\"oor_7\"),\n",
    "                    F.trim(F.col(\"value\").substr(4852,4)).alias(\"oor_8\"),\n",
    "                    F.trim(F.col(\"value\").substr(4842,5)).alias(\"oor_9\")         \n",
    "                )\n",
    "                .where((F.col(\"type\") == \"R\") | (F.col(\"type\") == \"M\"))\n",
    "                .withColumn(\n",
    "                    \"parcel_id\",\n",
    "                    F.regexp_replace('parcel_id', r'^[0]*', '')\n",
    "                )\n",
    "                .withColumn(\n",
    "                    \"legal_description\",\n",
    "                    quinn.single_space(F.col(\"legal_description\"))\n",
    "                )\n",
    "                .withColumn(\n",
    "                    \"street_address\",\n",
    "                    quinn.single_space(\n",
    "                        F.regexp_replace(\n",
    "                            F.concat_ws(\n",
    "                                \" \",\n",
    "                                \"street_number\",\n",
    "                                \"street_pre_direction\",\n",
    "                                \"street_name\",\n",
    "                                \"street_post_direction\",\n",
    "                                \"unit\",\n",
    "                            ),\n",
    "                            '\"',\n",
    "                            \"\",  # regexp replacement values\n",
    "                        )\n",
    "                    ),\n",
    "                )\n",
    "                .withColumn(\n",
    "                    \"oor\",\n",
    "                    quinn.single_space(\n",
    "                        F.regexp_replace(\n",
    "                            F.concat_ws(\n",
    "                                \" \",\n",
    "                                \"oor_1\",\n",
    "                                \"oor_2\",\n",
    "                                \"oor_3\",\n",
    "                                \"oor_4\",\n",
    "                                \"oor_5\",\n",
    "                                \"oor_6\",\n",
    "                                \"oor_7\",\n",
    "                                \"oor_9\",\n",
    "                            ),\n",
    "                            '\"',\n",
    "                            \"\",  # regexp replacement values\n",
    "                        )\n",
    "                    ),\n",
    "                )\n",
    "        ).drop(\"oor_1\", \"oor_2\", \"oor_3\", \"oor_4\", \"oor_5\", \"oor_6\", \"oor_7\", \"oor_8\", \"oor_9\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ade_info = _transform_ade_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'df_ade_info' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e685856693cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load APPRAISAL_INFO.TXT to db\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_ade_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mongo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"collection\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"info\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_ade_info' is not defined"
     ]
    }
   ],
   "source": [
    "# load APPRAISAL_INFO.TXT to db\n",
    "df_ade_info.write.format(\"mongo\").mode(\"overwrite\").option(\"collection\", \"info\").save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read APPRAISAL_LAND_DETAIL.TXT\n",
    "def _transform_land():\n",
    "    return (\n",
    "        (spark.read.text(path_ade_land)\n",
    "            .select(\n",
    "                F.trim(F.col(\"value\").substr(1, 12)).cast(\"string\").alias(\"parcel_id\"),\n",
    "                #   Incorrect values, see readme\n",
    "                #   F.trim(F.col(\"value\").substr(29,10)).cast(\"string\").alias(\"land_use_code\"),\n",
    "                #   F.trim(F.col(\"value\").substr(39,25)).cast(\"string\").alias(\"land_use_desc\"),\n",
    "                F.trim(F.col(\"value\").substr(84,14)).cast(\"integer\").alias(\"lot_size_sqft\"),\n",
    "                F.trim(F.col(\"value\").substr(112,14)).cast(\"integer\").alias(\"lot_depth_ft\"),\n",
    "                F.trim(F.col(\"value\").substr(98,14)).cast(\"integer\").alias(\"lot_frontage_ft\")\n",
    "            )\n",
    "            .withColumn(\n",
    "                    \"parcel_id\",\n",
    "                    F.regexp_replace('parcel_id', r'^[0]*', '')\n",
    "                )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ade_land = _transform_land()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'df_ade_land' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-52dfab1e865a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load APPRAISAL_LAND_DETAIL.TXT to db\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_ade_land\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mongo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"collection\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"info\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_ade_land' is not defined"
     ]
    }
   ],
   "source": [
    "# load APPRAISAL_LAND_DETAIL.TXT to db\n",
    "df_ade_land.write.format(\"mongo\").mode(\"overwrite\").option(\"collection\", \"info\").save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read APPRAISAL_IMPROVEMENT_DETAIL.TXT\n",
    "def _transfrom_imp_detail():\n",
    "    w = Window.partitionBy(\"parcel_id\")\n",
    "    return(\n",
    "    \n",
    "        (spark.read.text(path_ade_imp_info)\n",
    "            .select(\n",
    "                F.trim(F.col(\"value\").substr(1,12)).alias(\"parcel_id\"),\n",
    "                F.trim(F.col(\"value\").substr(86,4)).cast(\"integer\").alias(\"year\"),\n",
    "                F.trim(F.col(\"value\").substr(94,15)).cast(\"integer\").alias(\"sqft\")\n",
    "            )\n",
    "            .withColumn(\n",
    "                    \"parcel_id\",\n",
    "                    F.regexp_replace('parcel_id', r'^[0]*', '')\n",
    "            )\n",
    "            .withColumn(\"structure_total_sqft\", F.sum(\"sqft\").over(w))\n",
    "            .withColumn(\"year_built\", F.sum(\"year\").over(w))\n",
    "        ).drop(\"sqft\", \"year\").drop_duplicates([\"parcel_id\"])\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ade_imp_info = _transfrom_imp_detail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'df_ade_imp_info' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-97b496adab27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load APPRAISAL_IMPROVEMENT_DETAIL.TXT to db\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_ade_imp_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mongo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"collection\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"info\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_ade_imp_info' is not defined"
     ]
    }
   ],
   "source": [
    "# load APPRAISAL_IMPROVEMENT_DETAIL.TXT to db\n",
    "df_ade_imp_info.write.format(\"mongo\").mode(\"overwrite\").option(\"collection\", \"info\").save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge datasets (if necassary)\n",
    "\n",
    "# Example assuming more than one dataset\n",
    "df_county = df_ade_info.join(df_ade_land, \"parcel_id\", \"left\").join(df_ade_imp_info, \"parcel_id\", \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add required columns\n",
    "county_parcels = df_county \\\n",
    "    .withColumn(\"county\", F.lit(county_vars[\"county\"])) \\\n",
    "        .withColumn(\n",
    "        \"fingerprint\", F.base64(F.concat(F.lit(county_vars[\"county\"]), \"parcel_id\"))\n",
    "        ) \\\n",
    "        .withColumn(\"state\", F.lit(county_vars[\"state\"])) \\\n",
    "        .withColumn(\"fips_code\", F.lit(county_vars[\"fips_code\"])) \\\n",
    "        .withColumn(\"pipeline_run_id\", F.lit(county_vars[\"pipeline_run_id\"])) \\\n",
    "        .withColumn(\"pipeline_run_time\", F.lit(county_vars[\"pipeline_run_time\"])) \\\n",
    "        .withColumn(\"dataset_date\", F.lit(county_vars[\"dataset_date\"])) \\\n",
    "        .withColumn(\"record_hash\", F.sha2(F.concat_ws(\"||\", *df_county.columns), 256)) \\\n",
    "    .drop_duplicates([\"fingerprint\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load final dataset into a mongo collection with schema validation\n",
    "county_parcels.write.format(\"mongo\").mode(\"append\").option(\"database\", schema_check_db).option(\"collection\", schema_check_coll).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['fingerprint', 'pipeline_run_id', 'pipeline_run_time', 'dataset_date', 'record_hash', 'county', 'fips_code', 'state', 'unit_type', 'unit_count', 'unit_number', 'zip_four', 'zoning', 'land_use_code', 'land_use_desc', 'state_use_code', 'municipal_use_code', 'rooms', 'bedrooms', 'whole_bath', 'half_bath', 'bathrooms', 'land_market_value', 'improvement_market_value', 'foundation', 'roof_material', 'roof_style', 'heating_type', 'heating_fuel', 'cooling_type', 'style', 'condition', 'flag']\n"
    }
   ],
   "source": [
    "# get missing fields and document why they're missing on county readme.md\n",
    "missing_fields = [f for f in schema.fieldNames() if f not in df_county.schema.fieldNames()]\n",
    "print(missing_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[]\n"
    }
   ],
   "source": [
    "# check for extra fields\n",
    "extra_fields = [f for f in county_parcels.schema.fieldNames() if f not in schema.fieldNames()]\n",
    "print(extra_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}